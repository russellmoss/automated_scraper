# Savvy Pirate v2.0

```
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∞‚£ø‚£∑‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£¥‚£ø‚£ø‚£ø‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚°î‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢ü‚£´‚£Ø‚£∑‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚°ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢ã‚£µ‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ß‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£§‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚£ô‚†ø‚†ø‚†ø‚¢ü‚£´‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚¢π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚¢∏‚£ø‚£ø‚£ø‚°è‚†â‚†ô‚¢ø‚£ø‚£ø‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚£ø‚°ø‚°ç‚†≥‚£Ñ‚°Ä‚¢Ä‚£ø‚£ø‚£ø‚£ø‚£Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢ø‚£ø‚¢ø‚°Ñ‚†∏‚°ø‚¢Ñ‚†õ‚£ò‚¢†‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ß‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚°û‚£º‚°ª‚°Ñ‚†≥‚°§‚†Ω‚†æ‚†ø‚†ø‚†ø‚¢õ‚£ª‚£ø‚£ø‚£ø‚£∑‚°Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£æ‚£ø‚£ø‚£Ñ‚†ô‚¢∂‚£∂‚£∂‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ß‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ü‚†õ‚†â‚¢â‚£Å‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£â‚°â‚†ô‚†õ‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£Ø‚£ª‚£ç‚°≤‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä
‚†Ä‚¢Ä‚°Ä‚£∂‚£§‚£å‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†ã‚£Å‚£§‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£∂‚£§‚£à‚†õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ü‚†õ‚†õ‚†Å‚†Ä‚†Ä
‚£∞‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ù‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ü‚£°‚£∂‚†ø‚¢õ‚£õ‚£â‚£≠‚£≠‚£§‚£§‚°¥‚†∂‚†∂‚†∂‚†∂‚¢≤‚£¥‚£§‚†≠‚†≠‚°≠‚£ü‚†ª‚†¶‚£ù‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚¢â‚£Ä‚£†‚£∂‚£ø‚£Ü‚†Ä‚†Ä
‚†π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ô‚†ª‚£ø‚£Æ‚£õ‚†ø‚£ø‚£ø‚£ø‚£´‚£µ‚°∂‚†ü‚£õ‚£ã‚£≠‚£≠‚£∂‚£∂‚£∂‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚£æ‚£Æ‚£Ω‚£ø‚£ø‚£ø‚†ø‚†ü‚†õ‚†â‚¢Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚°Ä
‚†Ä‚†à‚†ô‚†ã‚†Å‚†Ä‚†à‚†â‚†õ‚†≥‚£≠‚£õ‚¢∑‚£¶‚£∏‚£ø‚£Ø‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°è‚£ø‚†Ä‚†Ä‚†Ä‚£Ä‚£¥‚£æ‚£ø‚£ø‚£ø‚£ø‚°ü‚£ø‚£ø‚£ø‚£ø‚°á
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£æ‚£ø‚£ø‚†ø‚†ø‚¢ø‚£π‚£ø‚£ß‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚¢∏‚°è‚£Ä‚£¥‚£æ‚£ø‚£ø‚†ø‚†õ‚†â‚†Ä‚†Ä‚†Ä‚†à‚†õ‚†õ‚†â‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£¥‚†ø‚†õ‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ø‚¢ø‚£ø‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚£ø‚†£‚£ü‚°ª‚†ü‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£¥‚£æ‚£ø‚†à‚°ø‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†à‚†â‚†õ‚†ª‚†ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†õ‚†â‚†â‚†Ä‚†à‚†â‚†õ‚£ø‚£Ω‚°ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£Ä‚£º‚£ø‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£π‚°ü‚£ª‚£ø‚°É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢π‚£∑‚£§‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚¢Ä‚£¥‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢π‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∞‚£ø‚¢£‚°á‚£ø‚£∑‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£º‚£ø‚£ø‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†ü‚†ã‚†ô‚†õ‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚†è‚†Ä‚†à‚¢ø‚£ø‚£ø‚£ø‚£¶‚£Ñ‚£Ä‚£Ä‚£Ä‚£†‚£¥‚£ø‚£è‚°û‚¢ª‚£∏‚£ø‚£∑‚£Ñ‚†Ä‚†Ä‚£Ä‚£§‚†¥‚£æ‚£ø‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†π‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†à‚¢ø‚£ø‚£µ‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚†æ‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£è‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£Ä‚£º‚£ø‚°õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚£°‚£æ‚£Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ä‚†Ä‚¢†‚£∂‚£ø‚£ø‚£Ø‚£ø‚°á‚†Ä‚¢π‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚£§‚£¶‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚†Ä‚£ø‚£ø‚¢∏‚£ø‚£∂‚£§‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ñ‚£†‚£¥‚£æ‚£ø‚£ü‚£ø‚†ü‚†Å‚£ø‚°á‚†Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°è‚°á‚¢Ä‚£ø‚£ø‚†ô‚¢Æ‚£õ‚†ø‚£∑‚£¶‚£Ñ‚£Ä‚£Ä‚£Ä‚£†‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚£¥‚£∂‚£∂‚£æ‚£ø‚£ø‚°ø‚£õ‚£Ω‚†û‚†ã‚†Ä‚†Ä‚†Ä‚£ø‚£∑‚†Ä‚£ç‚†á‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†â‚°Ñ‚£∏‚£ø‚°ø‚†Ä‚†Ä‚†à‚†ô‚†Æ‚£ü‚†ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£π‚£ø‚£ø‚£ø‚¢µ‚°ø‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢ø‚£ø‚£¶‚£ø‚°∑‚£Ñ‚†ô‚†ø‚£ø‚¢π‚£ø‚£ø‚¢º‚°ø‚†ã‚£°‚£∂‚£≥‚£ø‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ø‚†¨‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ª‚£ø‚£ø‚£∑‚£ª‚¢ø‚£∂‚£¨‚£à‚£â‚£â‚£§‚£¥‚£ø‚£ª‚£æ‚£ø‚£ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢ø‚£ø‚£ø‚°ø‚†á‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ª‚£ø‚£ø‚£ø‚£ø‚°á‚£ø‚£á‚£ø‚¢π‚£ø‚£ø‚£ø‚£ø‚°ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†è‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ä‚°Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
```

LinkedIn competitive intelligence Chrome extension - scrape and track competitor connections automatically.

## Features

- üîç **Automated LinkedIn Scraping** - Multi-layer extraction strategy resilient to LinkedIn DOM changes
- üìä **Google Sheets Integration** - Automatic data sync to Google Sheets with weekly tab management
- üìò **Savvy Pirate Command Center (Live Sync Workbook)** - One master workbook drives Searches, Mappings, and Schedules with live sync
- ‚è∞ **Scheduled Scraping** - Per-source scheduling with automated execution
- üîî **Zapier Webhooks** - Real-time notifications for scraping events with category filtering
- üìà **Tab Comparison** - Find new entries between different date tabs
- üîÑ **Retry Queue** - Local-first data queue with automatic retry on failures
- üéØ **Source Mapping** - Map multiple sources to different output workbooks
- üìú **Execution History** - Track all scraping runs with live profile count updates
- üîó **Clickable Links** - Direct links to scraped data in Google Sheets from execution history

## How It Works

### Architecture Overview

The extension uses a **three-layer architecture**:

1. **Content Script** (`content/content.js`)
   - Injected into LinkedIn search pages
   - Scrapes profile data using multi-layer selector strategy
   - Extracts: Name, Title, Location, LinkedIn URL, Accreditations
   - Sends scraped data to service worker via messages

2. **Service Worker** (`background/service_worker.js`)
   - Main orchestration hub (always running in background)
   - Manages Google Service Account authentication (JWT-based, no user interaction)
   - Handles scheduling and queue processing
   - Coordinates manual and automated scraping
   - Sends notifications via webhooks

3. **Popup UI** (`popup/popup.js`)
   - User interface (persistent side panel)
   - Configuration and monitoring
   - Displays progress, schedules, execution history

### Data Flow

```
LinkedIn Page ‚Üí Content Script ‚Üí Service Worker ‚Üí Queue ‚Üí Google Sheets API ‚Üí Google Sheets
                                      ‚Üì
                                Webhook Notifications
```

### Scraping Process

1. **Navigation**: Extension navigates to LinkedIn search URL (or uses dedicated tab)
2. **Extraction**: Content script finds profile cards and extracts data using fallback selectors
3. **Validation**: Each profile is validated (name, valid LinkedIn URL)
4. **Queue**: Valid profiles are added to local queue
5. **Sync**: Queue processor syncs to Google Sheets (with retry on failure)
6. **Tracking**: Execution records track total profiles scraped

### Queue System

- **Local-First**: All scraped data is stored locally before syncing
- **Retry Logic**: Failed syncs retry with exponential backoff (up to 5 retries)
- **Failed Queue**: Items that exceed max retries are moved to failed queue
- **Automatic Processing**: Queue is processed every 30 seconds via alarm

### Scheduling System

- **Per-Source**: Each source (Connection) can have its own schedule
- **Weekly**: Schedules run once per week on specified day/time
- **Biweekly (optional)**: ‚Äú1st & 3rd week of month‚Äù (odd) or ‚Äú2nd & 4th week of month‚Äù (even)
- **Eastern Time**: All schedules use Eastern Time (America/New_York)
- **Automatic**: Service worker checks schedules every minute
- **Execution Records**: Each scheduled run creates an execution record

### Savvy Pirate Command Center (Live Sync Workbook)

The **Savvy Pirate Command Center** is a single ‚Äúmaster‚Äù Google Sheet workbook that becomes the **system of record** for:
- Searches to scrape
- Which output workbook each person/source maps to
- Per-person schedules (day/time/frequency)

#### Workbook tabs (MUST be named exactly)

- **`Searches`**
- **`Mapping and Schedules`**

> The extension can still fall back to `Sheet1!A:C` for legacy setups, but the intended setup is `Searches`.

#### Tab 1: `Searches` (columns A:C)

Each row defines one LinkedIn search to run:

- **Column A ‚Äî Source Connection**: The person/source name (ex: `Jeff Nash`)
- **Column B ‚Äî Target Job Title**: Display label for the search (ex: `Financial Advisor`)
- **Column C ‚Äî LinkedIn Search URL**: The exact LinkedIn people search URL to scrape

#### Tab 2: `Mapping and Schedules` (columns A:H)

Each row defines the output workbook + schedule settings for a person/source:

- **Column A ‚Äî Name**: The person/source name (ex: `Jeff Nash`)
- **Column B ‚Äî Sheet_URL**: The **output workbook** URL (recommended: full Google Sheets URL)
- **Column C ‚Äî Title**: A role/title field for your own reference (ex: `CEO & Co-Founder`)
- **Column D ‚Äî Company**: Optional reference
- **Column E ‚Äî Day**: Optional reference text
- **Column F ‚Äî Day of Week**: Use the sheet dropdown (Sunday‚ÄìSaturday)
- **Column G ‚Äî Time (24hr)**: **24-hour format**, ex: `08:30`, `14:05`, `23:45`
- **Column H ‚Äî Frequency**: Use the sheet dropdown:
  - `Every week`
  - `1st & 3rd week of month`
  - `2nd & 4th week of month`

#### Critical rule: Name matching must be EXACT

The extension joins these tabs by **exact string match**:
- `Searches!A (Source Connection)` **must match exactly** `Mapping and Schedules!A (Name)`

If they don‚Äôt match (extra spaces, different capitalization, suffix differences), the person will appear as **unmapped** or **unscheduled**.

#### Scheduling time zone (EST/ET)

All schedule execution is computed in **Eastern Time** (`America/New_York`).
So the ‚ÄúTime (24hr)‚Äù you enter is interpreted as **ET/EST**.

#### How to use Live Sync in the extension

In the side panel popup, use **üìò Workbook Configuration (Live Sync)**:

- **Load**: Validates workbook access, reads both tabs, builds `workbookConfig`, and updates:
  - `inputSheetId` (master workbook ID)
  - `sourceMapping` (Name ‚Üí output workbook ID)
  - `savedWorkbooks` (output workbooks list)
  - `schedules` (created/updated via scheduler)

- **Sync Now**: Re-reads the workbook, diffs changes, and applies updates (same targets as Load)

- **Sync interval**: Controls the background sync alarm (periodic polling). The extension will sync automatically at this interval (skips syncing while a scrape is actively running).

- **Clear**: Clears the stored `workbookConfig` and sync change list (does not delete your Google Sheets).

#### Troubleshooting Live Sync

- **Sync Now shows no changes / does nothing**
  - Confirm you clicked **Load** at least once (Load creates `workbookConfig`)
  - Confirm no scrape is running (sync is skipped while scraping is active)
  - Open the service worker console and check:
    - `chrome.storage.local.get(['workbookConfig','lastSyncChanges'], console.log)`

- **A person shows as unmapped / unscheduled**
  - Ensure exact name match:
    - `Searches!A (Source Connection)` must match `Mapping and Schedules!A (Name)` **exactly**
    - Watch for trailing spaces, different capitalization, ‚ÄúJr.‚Äù vs no suffix, etc.

- **Sheet_URL mapping doesn‚Äôt work**
  - `Mapping and Schedules!B (Sheet_URL)` should be a full Google Sheets URL (recommended)
  - If you paste an ID instead, ensure it‚Äôs the raw spreadsheet ID (no extra characters)

- **Workbook name displays incorrectly**
  - The ‚ÄúTitle‚Äù column (Mapping and Schedules column C) is **NOT** the workbook name (it‚Äôs a role like ‚ÄúCEO & Co-Founder‚Äù)
  - The extension attempts to fetch the real output workbook title via the Sheets API
  - If the output workbook isn‚Äôt shared with the same Google account used for OAuth, the extension can‚Äôt read its title

- **Schedules not running when expected**
  - Time must be **24-hour format** in `Mapping and Schedules!G` (ex: `08:30`, `14:05`)
  - Schedules run in **Eastern Time** (`America/New_York`)
  - Frequency must match dropdown labels exactly:
    - `Every week`
    - `1st & 3rd week of month`
    - `2nd & 4th week of month`

- **Service Account / Authentication errors**
  - Ensure service account JSON key is properly configured in the extension popup (üîë Google Service Account section)
  - Verify the service account email has access to all Google Sheets (master workbook + output workbooks)
  - Check service worker console for detailed error messages
  - **Recommendation**: Use Service Account authentication for better reliability (no OAuth popups or expiry issues)

### Webhook Notifications

All notifications include:
- `type`: Specific notification type (schedule_started, scrape_failed, etc.)
- `category`: "status" or "error" (for easy Zapier filtering)
- `sourceName`: Connection Source name from Input Sheet
- `timestamp`: ISO timestamp
- `data`: Type-specific data (profiles scraped, error messages, etc.)

## Prerequisites

Before using this extension, you need:

1. **Google Cloud Project** with:
   - Google Sheets API enabled
   - Google Drive API enabled
   - **Google Service Account** (recommended) OR OAuth 2.0 Client ID for legacy OAuth

2. **Chrome/Chromium Browser** (Manifest V3 compatible)

3. **Google Account** with access to Google Sheets (for sharing sheets with service account)

4. **Zapier Webhook URL** (optional, for notifications)

## Google Authentication Setup

Savvy Pirate uses **Google Service Account authentication** for reliable, non-interactive access to Google Sheets. This replaces the previous OAuth method which required browser sign-in and could expire during long scrapes.

### Why Service Account Authentication?

**Advantages over OAuth:**
- ‚úÖ **No user interaction** - Tokens refresh automatically, never expire mid-scrape
- ‚úÖ **Works reliably on Chromium/Pi** - No browser OAuth dependencies or hanging issues
- ‚úÖ **More robust** - No popups, no sign-in prompts, no token expiry interruptions
- ‚úÖ **Better for automation** - Perfect for scheduled scrapes on Raspberry Pi
- ‚úÖ **Same setup everywhere** - Works identically on Chrome and Chromium

**How It Works:**
1. Extension creates a JWT (JSON Web Token) signed with the service account private key
2. Exchanges JWT for an access token via Google's OAuth endpoint
3. Caches tokens in memory and auto-refreshes before expiry (5-minute buffer)
4. No browser popups or user interaction required

---

## Installation

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd automated_scraper
```

### Step 2: Create Your manifest.json

‚ö†Ô∏è **IMPORTANT**: `manifest.json` is in `.gitignore` for security.

1. Copy the template file:
   ```bash
   cp manifest-template.json manifest.json
   ```

2. The `manifest.json` file is already configured. **Service Account authentication doesn't require OAuth configuration in the manifest** - credentials are stored securely in extension storage.

### Step 3: Create a Google Service Account

1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Select your project (or create a new one)
3. **Enable APIs:**
   - Go to **APIs & Services** > **Library**
   - Search for "Google Sheets API" ‚Üí Click **Enable**
   - Search for "Google Drive API" ‚Üí Click **Enable**

4. **Create a Service Account:**
   - Go to **APIs & Services** > **Credentials**
   - Click **Create Credentials** > **Service Account**
   - Enter a name (e.g., "Savvy Pirate Extension")
   - Click **Create and Continue**
   - Skip optional steps (or add description if desired)
   - Click **Done**

5. **Create a JSON Key:**
   - Click on the service account you just created
   - Go to the **Keys** tab
   - Click **Add Key** > **Create new key**
   - Choose **JSON** format
   - Click **Create** (this downloads the JSON key file)
   - **‚ö†Ô∏è Save this file securely** - you'll need it in Step 4

6. **Note the Service Account Email:**
   - The service account email looks like: `savvy-pirate-extension@your-project.iam.gserviceaccount.com`
   - You'll need to share your Google Sheets with this email address (see Step 5)

### Step 4: Configure Service Account in Extension

1. **Load the extension:**
   - Open Chrome/Chromium and go to `chrome://extensions` (or `chromium://extensions`)
   - Enable **Developer mode** (toggle in top-right)
   - Click **Load unpacked**
   - Select the extension directory

2. **Open the extension popup:**
   - Click the extension icon to open the side panel
   - You should see **üîë Google Service Account** section at the top

3. **Get your JSON key content:**
   - **If on your computer**: Open the downloaded JSON file in a text editor
   - **If on Raspberry Pi**: SSH in and copy it:
     ```bash
     ssh savvy-pirate@100.123.156.60 "cat /home/savvy-pirate/extensions/config/service-account-key.json"
     ```
   - Copy the **entire JSON content** (it should start with `{"type": "service_account", ...}`)

4. **Paste and save:**
   - In the extension popup, paste the JSON into the textarea
   - Click **Save Credentials**
   - You should see: "Service account configured: [email]"
   - The status should change to "Connected" with a green indicator

5. **Test the connection:**
   - Click **Test Connection**
   - Should show "‚úì Connection successful! Token obtained."

### Step 5: Share Google Sheets with Service Account

**‚ö†Ô∏è CRITICAL**: The service account needs access to all Google Sheets it will read/write.

**For each Google Sheet** (master workbook, output workbooks):

1. **Open the Google Sheet**
2. **Click Share** (top-right button)
3. **Add the Service Account Email:**
   - In the "Add people and groups" field, paste the **Service Account Email** (from Step 3.6)
   - It looks like: `savvy-pirate-extension@your-project.iam.gserviceaccount.com`
4. **Set permission to Editor** (or at least **Viewer** for read-only sheets)
5. **Click Send** (you can uncheck "Notify people" if desired)

**Verify access:**
- The service account email should appear in the "People with access" list
- You can test by trying to load a workbook in the extension

**Pro tip**: You can find the service account email in the extension popup after saving credentials - it's displayed under "Connected" status.

### Step 6: Test the Extension

1. Open the extension popup
2. Go to **üìò Workbook Configuration (Live Sync)** section
3. Enter your master workbook URL or ID
4. Click **Load**
5. The workbook should load **without any authentication prompts**
6. Check the service worker console (`chrome://extensions` ‚Üí Inspect views: service worker) for:
   - `[AUTH] Token obtained successfully` or `[AUTH] Using cached token`
   - No OAuth popups or sign-in prompts

## Platform-Specific Notes

### Service Account Authentication (All Platforms)

**Service Account authentication works identically on Chrome (Windows/Mac) and Chromium (Linux/Raspberry Pi).** There are no platform-specific differences - it's the same setup process everywhere.

**Benefits:**
- ‚úÖ Same setup on Chrome and Chromium
- ‚úÖ No browser-specific OAuth issues
- ‚úÖ No extension ID dependencies
- ‚úÖ Works reliably on Raspberry Pi without hanging or sign-in prompts

### Legacy OAuth (Not Recommended)

If you need to use the old OAuth method (not recommended), the extension may still support it, but **Service Account authentication is strongly recommended** for better reliability and no user interaction requirements.

#### IMPORTANT: Extension ID stability (for legacy OAuth only)

If using legacy OAuth, your OAuth redirect URI uses the extension ID:

- `https://<EXTENSION_ID>.chromiumapp.org/`

To keep the **same extension ID** on the Pi:

- Always load the extension from the **same folder path** (shown in `chrome://extensions` ‚Üí Details ‚Üí "Loaded from")
- When updating code, **copy files into that same folder** and click **Reload** (do not "Load unpacked" again)

## Raspberry Pi VNC Setup for Headless Operation

If you‚Äôre running the extension on a Raspberry Pi 24/7, VNC lets you remotely manage Chromium + the extension from your development PC.

### On the Raspberry Pi:

1. **Enable VNC via raspi-config:**

```bash
sudo raspi-config
# Navigate to: Interface Options ‚Üí VNC ‚Üí Enable
```

2. **Or install RealVNC Server manually:**

```bash
sudo apt update
sudo apt install realvnc-vnc-server realvnc-vnc-viewer
sudo systemctl enable vncserver-x11-serviced
sudo systemctl start vncserver-x11-serviced
```

3. **Find Pi's IP address:**

```bash
hostname -I
```

4. **Set a static IP (recommended):**
   - Edit `/etc/dhcpcd.conf` or configure via your router's DHCP reservation

### On your PC (Windows/Mac):

1. Download and install [RealVNC Viewer](https://www.realvnc.com/en/connect/download/viewer/)
2. Connect using: `<PI_IP_ADDRESS>:5900` (e.g., `192.168.3.232:5900`)
3. Login with your Pi username and password

### First-Time Extension Setup via VNC:

1. Open Chromium on the Pi
2. Go to `chrome://extensions` ‚Üí Enable Developer Mode ‚Üí Load unpacked
3. Complete Google OAuth sign-in (one-time)
4. Login to LinkedIn (one-time)
5. Configure schedules and sources
6. Extension will run 24/7 even when VNC is disconnected

## Deploying Code Updates to Raspberry Pi

When you make changes on your development machine, you can push updated extension code to the Pi and reload the extension in Chromium.

### Recommended: use the included deploy scripts (keeps the same extension ID)

This repo includes:

- `deploy-to-pi.sh` (bash; runs scp/ssh)
- `Deploy-to-Pi.bat` (one-click Windows wrapper; runs `deploy-to-pi.sh --files-only`)

#### Key rule (to avoid a new extension ID)

Deploy updates into the **same folder** Chromium is already using on the Pi:

- On the Pi, open `chrome://extensions` ‚Üí Savvy Pirate ‚Üí Details
- Confirm: **Loaded from: `~/extensions`**
- Ensure `PI_EXTENSION_PATH="/home/savvy-pirate/extensions"` in `deploy-to-pi.sh`

#### Typical workflow

1. On Windows, run `Deploy-to-Pi.bat` (copies files only)
2. On the Pi, open `chrome://extensions`
3. Click **Reload** on the existing Savvy Pirate extension

### deploy-to-pi.sh options

From Git Bash on Windows:

```bash
./deploy-to-pi.sh --files-only   # copy files only (recommended default)
./deploy-to-pi.sh --quick        # copy frequently edited files + restart chromium
./deploy-to-pi.sh --restart      # restart chromium only
./deploy-to-pi.sh --status       # show chromium status + extension folder listing
```

### Branch workflow (deploy from the branch you‚Äôre working on)

If you‚Äôre developing on a feature branch:

```bash
git checkout <your-branch>
git pull
./deploy-to-pi.sh --files-only
```

Then reload the extension on the Pi in `chrome://extensions`.

### Using SCP (Secure Copy)

**From Windows (PowerShell or CMD):**

```bash
scp -r "C:\Users\<USERNAME>\path\to\automated_scraper" <PI_USER>@<PI_IP>:~/extensions/
```

**Example:**

```bash
scp -r "C:\Users\russe\automated_scraper" savvy-pirate@192.168.3.232:~/extensions/
```

**From Mac/Linux:**

```bash
scp -r ~/path/to/automated_scraper <PI_USER>@<PI_IP>:~/extensions/
```

### After Deploying:

1. Connect via VNC
2. Go to `chrome://extensions`
3. Click the refresh icon on Savvy Pirate to reload the extension
4. Verify in service worker console that the new version loaded

### Optional: Create a Deploy Script

**Windows (deploy.bat):**

```batch
@echo off
scp -r "C:\Users\russe\automated_scraper" savvy-pirate@192.168.3.232:~/extensions/
echo Deployed! Remember to reload the extension in Chromium.
pause
```

**Mac/Linux (deploy.sh):**

```bash
#!/bin/bash
scp -r ~/automated_scraper savvy-pirate@192.168.3.232:~/extensions/
echo "Deployed! Remember to reload the extension in Chromium."
```

### SSH Access (for troubleshooting):

```bash
ssh savvy-pirate@192.168.3.232
```

## Usage

### First-Time Setup

1. **Load Input Sheet**
   - Open the extension side panel (click extension icon)
   - Enter your Google Sheet URL or ID in "Input Sheet" section
   - Click "Load"
   - Your input sheet should have columns: Source Connection (column A), Target Job Title (column B), LinkedIn Search URL (column C)
   - The input sheet URL is automatically saved and persists across sessions

2. **Add Output Workbook**
   - Click "+ Add Workbook" in "Workbook Mappings" section
   - Enter Google Sheet URL or ID
   - Optionally map to a specific source
   - Click "Add Workbook"

3. **Map Sources to Workbooks**
   - In "Workbook Mappings", select a workbook for each source
   - Data from each source will be saved to its mapped workbook
   - Each source can have its own workbook

### Manual Scraping

Manual scraping automatically navigates through all searches for a selected source:

1. Open the extension side panel
2. Go to "üîç Manual Scraping" section
3. Select a source from the dropdown
4. Click "‚ñ∂ Start Scrape"
5. The extension will:
   - Navigate to the first search URL for that source
   - Scrape all profiles from all pages
   - Automatically move to the next search URL
   - Continue until all searches are complete
6. Monitor progress in real-time:
   - Progress bar shows completed/total searches
   - Execution history shows live profile count updates
7. Click "‚èπ Stop" to stop scraping at any time
8. All scraping occurs in a single dedicated tab (no new tabs opened)

**Note**: You must be logged into LinkedIn. The extension uses a dedicated browser tab for all scraping operations.

### Scheduled Scraping

1. Open the extension side panel
2. Go to "‚è∞ Schedules" section
3. Click "+ Add Schedule"
4. Select:
   - Source name (Connection Source from Input Sheet)
   - Day of week
   - Time (hour and minute) - uses Eastern Time
5. Click "Save Schedule"
6. The schedule will run automatically at the specified time
7. Toggle schedules on/off using the switch
8. View execution history in "üìú Execution History" section
9. Execution history shows:
   - Date and time
   - Source name (clickable link to scraped data)
   - Status (running/completed/failed)
   - Profiles scraped (updates live during scraping)
   - Click on source name or profile count to open the Google Sheets tab

### Compare Tabs

Compare two date tabs to find new entries:

1. Open the extension side panel
2. Go to "üìä Compare Tabs" section
3. Select a workbook
4. Select baseline tab (older data) and compare tab (newer data)
5. Enter output tab name (e.g., "New_12_16")
6. Select compare key:
   - **Name**: Compare by person's name
   - **LinkedIn URL**: Compare by LinkedIn profile URL (recommended, more accurate)
7. Click "Compare Tabs"
8. New tab will be created with only the differential entries (entries in compare tab but not in baseline tab)

### Settings

**Zapier Webhook**:
1. Go to "‚öôÔ∏è Settings" section
2. Enter your Zapier webhook URL
3. Click "Save Webhook URL"
4. Click "Test Webhook" to verify (sends a test `schedule_started` notification)

**Webhook Notification Categories**:

All notifications include a `category` field for easy filtering in Zapier:

- **"status"**: Regular notifications (schedule_started, schedule_completed)
- **"error"**: Error notifications (schedule_failed, scrape_failed, error)

**Recommended Zapier Setup**:

1. Use **Paths** in Zapier to branch notifications
2. Path 1: Filter `category` equals `"status"` ‚Üí Route to Status Channel
3. Path 2: Filter `category` equals `"error"` ‚Üí Route to Error Channel

**All webhook payloads include**:
- `type`: Notification type
- `category`: "status" or "error"
- `sourceName`: Connection Source name from Input Sheet (column A)
- `timestamp`: ISO timestamp
- `data`: Type-specific data

## Data Formats

### Input Sheet Format

The input sheet should have three columns:

| Column A | Column B | Column C |
|----------|----------|----------|
| Source Connection | Target Job Title | LinkedIn Search URL |

Example:
```
Taylor Matthews | Financial Advisor | https://www.linkedin.com/search/results/people/?origin=FACETED_SEARCH&connectionOf=...
Jeff Nash | Wealth Manager | https://www.linkedin.com/search/results/people/...
```

- **Column A (Source Connection)**: The name of the person whose connections you're scraping
- **Column B (Target Job Title)**: The job title to search for
- **Column C (LinkedIn Search URL)**: The full LinkedIn People search URL

**Multiple rows with the same Source Connection name** will all be scraped together when that source is selected.

### Output Sheet Format

Each workbook automatically creates weekly tabs named `MM_DD_YY` (Eastern Time):

| Date | Name | Title | Location | Connection Source | LinkedIn URL | Accreditation 1-6 |
|------|------|-------|----------|-------------------|--------------|-------------------|
| YYYY-MM-DD | John Doe, CFP¬Æ | Financial Advisor | New York, NY | Taylor Matthews | https://... | CFA |

**Columns**:
1. **Date**: Date scraped (YYYY-MM-DD)
2. **Name**: Person's name (accreditations extracted to separate columns)
3. **Title**: Job title
4. **Location**: Location
5. **Connection Source**: Source Connection from Input Sheet
6. **LinkedIn URL**: Full LinkedIn profile URL
7. **Accreditation 1-6**: Professional accreditations (CFA, CFP¬Æ, MBA, etc.)

**Tab Naming**: Output tabs are automatically named `MM_DD_YY` (Eastern Time), e.g., `12_17_24` for December 17, 2024.

## Troubleshooting

### Extension Won't Load

- Check that `manifest.json` is valid JSON
- Verify all required files exist (service worker, content script, popup)
- Check Chrome console for errors
- Ensure you created `manifest.json` from `manifest-template.json`

### Service Account Authentication Issues

#### "Service account not configured" error

- **Solution**: Go to extension popup ‚Üí **üîë Google Service Account** section ‚Üí Paste your JSON key and click **Save Credentials**
- Verify the JSON is valid: It should start with `{"type": "service_account", ...}` and contain `client_email`, `private_key`, and `token_uri` fields

#### "Token exchange failed" or "401 Unauthorized"

- **Cause**: Service account doesn't have access to the Google Sheet
- **Solution**: Share the Google Sheet with the service account email:
  1. Open the Google Sheet
  2. Click **Share** (top-right)
  3. Add the service account email (looks like `savvy-pirate-extension@your-project.iam.gserviceaccount.com`)
  4. Set permission to **Editor** (or **Viewer** for read-only)
  5. Click **Send**

#### "Cannot access workbook" when loading

- **Cause**: Service account email not shared with the workbook
- **Solution**: Share the workbook with the service account email (see above)
- **Verify**: Check the service account email in the popup (should show "Connected" with the email address)

#### Test Connection fails

- Check service worker console for detailed error messages
- Verify JSON key is complete and valid JSON
- Ensure Google Sheets API and Drive API are enabled in Google Cloud Console
- Verify service account has proper permissions in Google Cloud Console

### Legacy OAuth Errors (Not Recommended - Use Service Account Instead)

If you're still using OAuth (not recommended), these troubleshooting steps may help:

- Verify Extension ID matches in Google Cloud Console
- Check redirect URI format: `https://<extension-id>.chromiumapp.org/`
- Re-authenticate by clicking extension icon
- Ensure OAuth Client ID in `manifest.json` matches Google Cloud Console
- Verify OAuth consent screen is configured correctly

#### OAuth keeps prompting / "The user is not signed in" (Raspberry Pi Chromium)

**Recommendation**: Switch to Service Account authentication (see Step 3 in Installation). It's more reliable and doesn't require browser OAuth.

If you must use OAuth:
1. On the Pi, open `chrome://extensions` ‚Üí Savvy Pirate ‚Üí copy the **ID**
2. In Google Cloud Console ‚Üí **APIs & Services ‚Üí Credentials**, confirm you are using a **Web application** OAuth client
3. In that same OAuth client, confirm **Authorized redirect URIs** includes:
   - `https://<EXTENSION_ID>.chromiumapp.org/` (replace with the ID from step 1)
4. Confirm the Pi's deployed `manifest.json` uses that **Web application client ID** (not a Chrome extension client)
5. Reload the extension (`chrome://extensions` ‚Üí Reload) and try again

### Service Worker Inactive

- Click extension icon to wake it
- Check service worker console: `chrome://extensions` ‚Üí "service worker" link
- Service worker should automatically wake when extension is used

### Scraping Finds 0 Profiles

LinkedIn frequently changes their DOM structure. To fix:

1. Navigate to a LinkedIn search page
2. Open browser console (F12)
3. Copy the content of `linkedin-diagnostic.js` and paste in console
4. Review output to identify new selectors
5. Update `SELECTORS` object in `content/content.js`
6. Reload extension

### Duplicate Rows or Invalid Data

If you see duplicate rows or rows with empty names:

1. The extension validates all profiles before adding to queue
2. Invalid profiles (empty names, invalid URLs) are skipped
3. Duplicates are prevented by URL-based deduplication
4. Use the "Deduplicate" feature in the popup footer if needed

### Data Not Appearing in Sheets

1. Check queue status in popup footer
2. Check for failed items (will show in footer)
3. Click "Retry Failed" to retry failed items
4. Check service worker console for errors
5. Verify workbook permissions (extension needs edit access)
6. Check Google Sheets API quota in Google Cloud Console

### Schedule Not Triggering

- Verify schedule is enabled (toggle switch in Schedules section)
- Check timezone (schedules use Eastern Time)
- Check service worker console for schedule logs
- Verify alarms are created: See debugging commands below
- Schedule checker runs every minute, so may take up to 1 minute after scheduled time

### Webhook Not Working

- Verify webhook URL is correct
- Check that Zapier webhook is active
- Test using "Test Webhook" button in Settings
- Check service worker console for webhook errors
- Verify `https://hooks.zapier.com/*` is in host_permissions (already included)

### Execution History Not Updating

- Execution history updates automatically when profiles are scraped
- If profile count isn't updating, check service worker console
- Reload the extension side panel to refresh the view
- Execution history is limited to last 100 records

## Debugging

### Service Worker Console

1. Go to `chrome://extensions`
2. Find "Savvy Pirate"
3. Click "service worker" link (opens DevTools)

### View Storage

In service worker console:
```javascript
chrome.storage.local.get(null, data => console.table(data));
```

### Check Alarms

In service worker console:
```javascript
chrome.alarms.getAll(alarms => {
    console.log('Active alarms:', alarms.map(a => ({
        name: a.name,
        next: new Date(a.scheduledTime).toLocaleString()
    })));
});
```

### Test Message Handlers

In service worker console:
```javascript
// Test ping
chrome.runtime.sendMessage({action: 'PING'}, console.log);

// Get queue status
chrome.runtime.sendMessage({action: 'GET_QUEUE_STATUS'}, console.log);

// Get schedules
chrome.runtime.sendMessage({action: 'GET_SCHEDULES'}, console.log);

// Get execution history
chrome.runtime.sendMessage({action: 'GET_EXECUTION_HISTORY', limit: 10}, console.log);
```

### Content Script Test

On a LinkedIn page, open browser console (F12):
```javascript
chrome.runtime.sendMessage({action: 'PING'}, r => console.log('Content script alive:', r));
```

## File Structure

```
automated_scraper/
‚îú‚îÄ‚îÄ manifest.json              # ‚ö†Ô∏è DO NOT COMMIT - contains your OAuth Client ID
‚îú‚îÄ‚îÄ manifest-template.json     # ‚úÖ Template for creating manifest.json
‚îú‚îÄ‚îÄ .gitignore                 # Excludes manifest.json and sensitive files
‚îú‚îÄ‚îÄ background/
‚îÇ   ‚îú‚îÄ‚îÄ service_worker.js      # Main orchestration hub
‚îÇ   ‚îú‚îÄ‚îÄ auth.js                # OAuth token management
‚îÇ   ‚îú‚îÄ‚îÄ sheets_api.js          # Google Sheets API wrapper
‚îÇ   ‚îú‚îÄ‚îÄ sync_queue.js          # Local queue with retry logic
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.js           # Schedule management and execution history
‚îÇ   ‚îî‚îÄ‚îÄ notifications.js       # Zapier webhook notifications
‚îú‚îÄ‚îÄ content/
‚îÇ   ‚îî‚îÄ‚îÄ content.js             # LinkedIn DOM scraping
‚îú‚îÄ‚îÄ popup/
‚îÇ   ‚îú‚îÄ‚îÄ popup.html             # UI structure
‚îÇ   ‚îú‚îÄ‚îÄ popup.css              # Dark theme styling
‚îÇ   ‚îî‚îÄ‚îÄ popup.js               # UI controller
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ constants.js           # Shared constants
‚îú‚îÄ‚îÄ icons/
‚îÇ   ‚îú‚îÄ‚îÄ icon16.png
‚îÇ   ‚îú‚îÄ‚îÄ icon48.png
‚îÇ   ‚îî‚îÄ‚îÄ icon128.png
‚îú‚îÄ‚îÄ linkedin-diagnostic.js     # Selector maintenance tool
‚îî‚îÄ‚îÄ README.md                  # This file
```

## Log Prefixes

When debugging, look for these log prefixes in the service worker console:

- `[SW]` - Service worker (main orchestration)
- `[AUTH]` - Authentication
- `[QUEUE]` - Sync queue operations
- `[SHEETS]` - Sheets API calls
- `[SCHEDULE]` - Scheduling operations
- `[NOTIFY]` - Webhook notifications
- `[CS]` - Content script (in browser console on LinkedIn pages)
- `[POPUP]` - Popup UI

## Rate Limiting & Detection Prevention

### Current Throttling

The extension includes built-in anti-detection delays:

**Between Pages** (within a search):
- Random delay: **5-8 seconds** between pages
- Scroll wait: **2 seconds** for lazy loading

**Between Searches**:
- **Manual scraping**: 5-10 seconds
- **Auto-run/scheduled**: 30-60 seconds

**Other Timing**:
- Page load wait: 3-5 seconds
- Scroll wait: 2 seconds

### Recommendations for Safe Scraping

‚ö†Ô∏è **Important**: Even with a Pro/Recruiter account, LinkedIn can still detect automated scraping patterns. Account type doesn't exempt you from detection, but may provide higher rate limits.

**Best Practices**:

1. **Limit Searches Per Session**:
   - **Conservative**: 10-20 searches per hour
   - **Moderate**: 20-50 searches per hour  
   - **Aggressive** (not recommended): 50+ searches per hour

2. **Spread Out Scraping**:
   - Use scheduled scraping instead of running everything at once
   - Schedule different sources at different times
   - Avoid scraping the same source multiple times per day

3. **Respect Daily Limits**:
   - Limit total searches per day (recommended: < 100/day)
   - Take breaks between large scraping sessions

4. **Watch for Warning Signs**:
   - CAPTCHAs appearing frequently
   - "Something went wrong" errors
   - Account restrictions or warnings from LinkedIn
   - IP-based rate limiting

5. **Use Scheduled Scraping**:
   - Spread searches throughout the week
   - One source per day is safer than multiple sources in one day
   - Schedule during normal business hours (LinkedIn may be more lenient)

### Adjusting Throttling

You can increase delays in `utils/constants.js`:

```javascript
export const CONFIG = {
    MIN_WAIT_SECONDS: 5,      // Increase to 8-10 for more conservative
    MAX_WAIT_SECONDS: 8,      // Increase to 15-20 for more conservative
    // ...
};
```

For search delays, modify in `background/service_worker.js`:
- **Manual scrape**: Currently 5-10 seconds (line ~528)
- **Auto-run**: Currently 30-60 seconds (line ~415)

### Pro/Recruiter Account Considerations

**Advantages**:
- Higher search result limits
- More InMail credits
- Access to Recruiter tools

**Limitations**:
- **Does NOT prevent detection** of automated scraping
- LinkedIn still monitors for bot-like behavior patterns
- Account can still be restricted or banned for scraping

**Recommendation**: Treat your account with care regardless of type. The extension's throttling is designed to mimic human behavior, but excessive scraping can still trigger LinkedIn's anti-scraping systems.

## Security Notes

‚ö†Ô∏è **IMPORTANT**:

### Service Account JSON Key Security

- **Service Account JSON Key contains a private key** - **NEVER commit this to git or share publicly**
- The JSON key is stored in `chrome.storage.local` (encrypted by Chrome)
- Only paste it into the extension popup on trusted devices
- If compromised, delete the service account in Google Cloud Console and create a new one
- The private key in the JSON file allows full access to Google Sheets - treat it as highly sensitive

### Service Account Email

- The service account email (e.g., `savvy-pirate-extension@your-project.iam.gserviceaccount.com`) is **public and safe to share**
- Share this email with Google Sheets to grant access
- The email itself doesn't grant access - the private key does

### Legacy OAuth (if still using)

- `manifest.json` may contain your OAuth Client ID - keep it secure
- **Do NOT commit `manifest.json` to public repositories**
- `.gitignore` is configured to exclude sensitive files
- Use `manifest-template.json` as a template for others
- OAuth Client ID allows access to Google Sheets - treat it as a secret

## Key Features Explained

### Dedicated Scrape Tab

The extension uses a single dedicated browser tab for all scraping operations. This tab is:
- Created once and reused for all scraping sessions
- Saved in extension storage
- Automatically navigated to new search URLs
- Prevents opening multiple tabs during scraping

### Live Execution History

Execution history updates in real-time during scraping:
- Profile count updates as profiles are scraped
- Source name is clickable to open the Google Sheets tab
- Profile count is also clickable when > 0
- Links work even while scraping is running

### Weekly Tab Management

Output workbooks automatically create weekly tabs:
- Tab names: `MM_DD_YY` (Eastern Time)
- Tabs are created on first scrape of the week
- Headers are automatically added
- Old tabs are preserved (not deleted)

### Multi-Layer Selector Strategy

The content script uses a fallback selector strategy:
- Primary selectors (most reliable)
- Fallback selectors (backup if primary fails)
- Validation ensures data quality
- Prevents scraping invalid profiles

### Queue Retry System

Failed syncs are automatically retried:
- Exponential backoff (2s, 4s, 8s, 16s, 32s)
- Maximum 5 retries
- Failed items moved to failed queue after max retries
- Manual retry available from popup footer

## Support

For issues or questions:
1. Check troubleshooting section above
2. Review service worker console logs
3. Use `linkedin-diagnostic.js` if scraping fails
4. Check Google Sheets API quota if data sync fails
5. Review execution history for detailed error information

## License

[Add your license here]

---

**Version**: 2.0.0  
**Last Updated**: December 2024
