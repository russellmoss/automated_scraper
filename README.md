# Savvy Pirate v2.0

```
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∞‚£ø‚£∑‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£¥‚£ø‚£ø‚£ø‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£∂‚£¶‚£§‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚°î‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢ü‚£´‚£Ø‚£∑‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£†‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚°ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢ã‚£µ‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ß‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£§‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£¶‚£ô‚†ø‚†ø‚†ø‚¢ü‚£´‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚¢π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£§‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚¢∏‚£ø‚£ø‚£ø‚°è‚†â‚†ô‚¢ø‚£ø‚£ø‚£¶‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚£ø‚°ø‚°ç‚†≥‚£Ñ‚°Ä‚¢Ä‚£ø‚£ø‚£ø‚£ø‚£Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚¢Ä‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢ø‚£ø‚¢ø‚°Ñ‚†∏‚°ø‚¢Ñ‚†õ‚£ò‚¢†‚£º‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ß‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚°û‚£º‚°ª‚°Ñ‚†≥‚°§‚†Ω‚†æ‚†ø‚†ø‚†ø‚¢õ‚£ª‚£ø‚£ø‚£ø‚£∑‚°Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£æ‚£ø‚£ø‚£Ñ‚†ô‚¢∂‚£∂‚£∂‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ß‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ü‚†õ‚†â‚¢â‚£Å‚£Ä‚£Ä‚£Ä‚£Ä‚£Ä‚£â‚°â‚†ô‚†õ‚†ª‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£Ø‚£ª‚£ç‚°≤‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä
‚†Ä‚¢Ä‚°Ä‚£∂‚£§‚£å‚¢ª‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†ã‚£Å‚£§‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£∂‚£§‚£à‚†õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ü‚†õ‚†õ‚†Å‚†Ä‚†Ä
‚£∞‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ù‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ü‚£°‚£∂‚†ø‚¢õ‚£õ‚£â‚£≠‚£≠‚£§‚£§‚°¥‚†∂‚†∂‚†∂‚†∂‚¢≤‚£¥‚£§‚†≠‚†≠‚°≠‚£ü‚†ª‚†¶‚£ù‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚¢â‚£Ä‚£†‚£∂‚£ø‚£Ü‚†Ä‚†Ä
‚†π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ô‚†ª‚£ø‚£Æ‚£õ‚†ø‚£ø‚£ø‚£ø‚£´‚£µ‚°∂‚†ü‚£õ‚£ã‚£≠‚£≠‚£∂‚£∂‚£∂‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚£æ‚£Æ‚£Ω‚£ø‚£ø‚£ø‚†ø‚†ü‚†õ‚†â‚¢Ä‚£¥‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚°Ä
‚†Ä‚†à‚†ô‚†ã‚†Å‚†Ä‚†à‚†â‚†õ‚†≥‚£≠‚£õ‚¢∑‚£¶‚£∏‚£ø‚£Ø‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°è‚£ø‚†Ä‚†Ä‚†Ä‚£Ä‚£¥‚£æ‚£ø‚£ø‚£ø‚£ø‚°ü‚£ø‚£ø‚£ø‚£ø‚°á
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£æ‚£ø‚£ø‚†ø‚†ø‚¢ø‚£π‚£ø‚£ß‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚¢∏‚°è‚£Ä‚£¥‚£æ‚£ø‚£ø‚†ø‚†õ‚†â‚†Ä‚†Ä‚†Ä‚†à‚†õ‚†õ‚†â‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£¥‚†ø‚†õ‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ø‚¢ø‚£ø‚¢∏‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚£ø‚†£‚£ü‚°ª‚†ü‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ä‚£¥‚£æ‚£ø‚†à‚°ø‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†à‚†â‚†õ‚†ª‚†ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†õ‚†â‚†â‚†Ä‚†à‚†â‚†õ‚£ø‚£Ω‚°ü‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£†‚£§‚£∂‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£Ä‚£º‚£ø‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£π‚°ü‚£ª‚£ø‚°É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢π‚£∑‚£§‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚¢Ä‚£¥‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚¢π‚£ø‚£ø‚£ø‚°Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£∞‚£ø‚¢£‚°á‚£ø‚£∑‚°Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£º‚£ø‚£ø‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†ü‚†ã‚†ô‚†õ‚†ª‚£ø‚£ø‚£ø‚£ø‚£ø‚†è‚†Ä‚†à‚¢ø‚£ø‚£ø‚£ø‚£¶‚£Ñ‚£Ä‚£Ä‚£Ä‚£†‚£¥‚£ø‚£è‚°û‚¢ª‚£∏‚£ø‚£∑‚£Ñ‚†Ä‚†Ä‚£Ä‚£§‚†¥‚£æ‚£ø‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†π‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†à‚¢ø‚£ø‚£µ‚£æ‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†Ä‚†Ä‚†Ä‚†π‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£∂‚†æ‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚£è‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚£Ä‚£º‚£ø‚°õ‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚†Ä‚†Ä‚†Ä‚†Ä‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ü‚£°‚£æ‚£Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ò‚†Ä‚†Ä‚¢†‚£∂‚£ø‚£ø‚£Ø‚£ø‚°á‚†Ä‚¢π‚£ø‚£ø‚£ø‚£ø‚£∑‚£§‚£§‚£¶‚£∂‚£ø‚£ø‚£ø‚£ø‚£ø‚°á‚†Ä‚£ø‚£ø‚¢∏‚£ø‚£∂‚£§‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ñ‚£†‚£¥‚£æ‚£ø‚£ü‚£ø‚†ü‚†Å‚£ø‚°á‚†Ä‚£ø‚°ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°è‚°á‚¢Ä‚£ø‚£ø‚†ô‚¢Æ‚£õ‚†ø‚£∑‚£¶‚£Ñ‚£Ä‚£Ä‚£Ä‚£†‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚£¥‚£∂‚£∂‚£æ‚£ø‚£ø‚°ø‚£õ‚£Ω‚†û‚†ã‚†Ä‚†Ä‚†Ä‚£ø‚£∑‚†Ä‚£ç‚†á‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚†ø‚†â‚°Ñ‚£∏‚£ø‚°ø‚†Ä‚†Ä‚†à‚†ô‚†Æ‚£ü‚†ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°Ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£π‚£ø‚£ø‚£ø‚¢µ‚°ø‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢ø‚£ø‚£¶‚£ø‚°∑‚£Ñ‚†ô‚†ø‚£ø‚¢π‚£ø‚£ø‚¢º‚°ø‚†ã‚£°‚£∂‚£≥‚£ø‚£ø‚£ø‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ø‚†¨‚£ø‚£ø‚£ø‚£ø‚£ø‚£∑‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†∏‚£ø‚£ø‚£ø‚£ø‚†ü‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ª‚£ø‚£ø‚£∑‚£ª‚¢ø‚£∂‚£¨‚£à‚£â‚£â‚£§‚£¥‚£ø‚£ª‚£æ‚£ø‚£ø‚†ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢ø‚£ø‚£ø‚°ø‚†á‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚†ã‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†ª‚£ø‚£ø‚£ø‚£ø‚°á‚£ø‚£á‚£ø‚¢π‚£ø‚£ø‚£ø‚£ø‚°ü‚†Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†à‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†ã‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†ô‚¢ø‚£ø‚£ø‚£ø‚£ø‚°ø‚†è‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚°Ä‚°Å‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä
```

LinkedIn competitive intelligence Chrome extension - scrape and track competitor connections automatically.

## Features

- üîç **Automated LinkedIn Scraping** - Multi-layer extraction strategy resilient to LinkedIn DOM changes
- üìä **Google Sheets Integration** - Automatic data sync to Google Sheets with weekly tab management
- ‚è∞ **Scheduled Scraping** - Per-source scheduling with automated execution
- üîî **Zapier Webhooks** - Real-time notifications for scraping events with category filtering
- üìà **Tab Comparison** - Find new entries between different date tabs
- üîÑ **Retry Queue** - Local-first data queue with automatic retry on failures
- üéØ **Source Mapping** - Map multiple sources to different output workbooks
- üìú **Execution History** - Track all scraping runs with live profile count updates
- üîó **Clickable Links** - Direct links to scraped data in Google Sheets from execution history

## How It Works

### Architecture Overview

The extension uses a **three-layer architecture**:

1. **Content Script** (`content/content.js`)
   - Injected into LinkedIn search pages
   - Scrapes profile data using multi-layer selector strategy
   - Extracts: Name, Title, Location, LinkedIn URL, Accreditations
   - Sends scraped data to service worker via messages

2. **Service Worker** (`background/service_worker.js`)
   - Main orchestration hub (always running in background)
   - Manages OAuth authentication
   - Handles scheduling and queue processing
   - Coordinates manual and automated scraping
   - Sends notifications via webhooks

3. **Popup UI** (`popup/popup.js`)
   - User interface (persistent side panel)
   - Configuration and monitoring
   - Displays progress, schedules, execution history

### Data Flow

```
LinkedIn Page ‚Üí Content Script ‚Üí Service Worker ‚Üí Queue ‚Üí Google Sheets API ‚Üí Google Sheets
                                      ‚Üì
                                Webhook Notifications
```

### Scraping Process

1. **Navigation**: Extension navigates to LinkedIn search URL (or uses dedicated tab)
2. **Extraction**: Content script finds profile cards and extracts data using fallback selectors
3. **Validation**: Each profile is validated (name, valid LinkedIn URL)
4. **Queue**: Valid profiles are added to local queue
5. **Sync**: Queue processor syncs to Google Sheets (with retry on failure)
6. **Tracking**: Execution records track total profiles scraped

### Queue System

- **Local-First**: All scraped data is stored locally before syncing
- **Retry Logic**: Failed syncs retry with exponential backoff (up to 5 retries)
- **Failed Queue**: Items that exceed max retries are moved to failed queue
- **Automatic Processing**: Queue is processed every 30 seconds via alarm

### Scheduling System

- **Per-Source**: Each source (Connection) can have its own schedule
- **Weekly**: Schedules run once per week on specified day/time
- **Eastern Time**: All schedules use Eastern Time (America/New_York)
- **Automatic**: Service worker checks schedules every minute
- **Execution Records**: Each scheduled run creates an execution record

### Webhook Notifications

All notifications include:
- `type`: Specific notification type (schedule_started, scrape_failed, etc.)
- `category`: "status" or "error" (for easy Zapier filtering)
- `sourceName`: Connection Source name from Input Sheet
- `timestamp`: ISO timestamp
- `data`: Type-specific data (profiles scraped, error messages, etc.)

## Prerequisites

Before using this extension, you need:

1. **Google Cloud Project** with:
   - Google Sheets API enabled
   - Google Drive API enabled
   - OAuth 2.0 Client ID (**Web application** type) ‚úÖ Recommended (works on Raspberry Pi Chromium)
   - Authorized redirect URIs include: `https://<EXTENSION_ID>.chromiumapp.org/`
     - You get `<EXTENSION_ID>` from `chrome://extensions` after loading the unpacked extension

2. **Chrome Browser** (Manifest V3 compatible)

3. **Google Account** with access to Google Sheets

4. **Zapier Webhook URL** (optional, for notifications)

## Installation

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd automated_scraper
```

### Step 2: Create Your manifest.json

‚ö†Ô∏è **IMPORTANT**: `manifest.json` is in `.gitignore` for security because it contains your OAuth Client ID.

1. Copy the template file:
   ```bash
   cp manifest-template.json manifest.json
   ```

2. Edit `manifest.json` and replace `YOUR_CLIENT_ID.apps.googleusercontent.com` with your actual OAuth 2.0 Client ID from Google Cloud Console.

   Your `manifest.json` should look like this:
   ```json
   {
     "oauth2": {
       "client_id": "123456789-abc123xyz.apps.googleusercontent.com",
       ...
     }
   }
   ```

### Step 3: Get Your OAuth 2.0 Client ID

1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Select your project (or create a new one)
3. Navigate to **APIs & Services** > **Credentials**
4. Click **Create Credentials** > **OAuth client ID**
5. Choose **Chrome extension** as the application type
6. You'll need your Extension ID (see Step 4)
7. Copy the Client ID (it will look like: `123456789-abc123xyz.apps.googleusercontent.com`)
8. Add this Client ID to your `manifest.json`

### Step 4: Load the Extension in Chrome

1. Open Chrome and go to `chrome://extensions/`
2. Enable **Developer mode** (toggle in top right)
3. Click **Load unpacked**
4. Select the `automated_scraper` directory
5. Copy the **Extension ID** shown on the extensions page (you'll need this for OAuth setup)
6. Go back to Google Cloud Console and add this Extension ID to your OAuth credentials

### Step 5: Configure OAuth Consent Screen

1. In Google Cloud Console, go to **APIs & Services** > **OAuth consent screen**
2. Choose **Internal** or **External** (depending on your Google Workspace)
3. Fill in required fields:
   - App name: "Savvy Pirate"
   - User support email: Your email
   - Developer contact: Your email
4. Add scopes:
   - `https://www.googleapis.com/auth/spreadsheets`
   - `https://www.googleapis.com/auth/drive.file`
5. Save and continue

### Step 6: Test the Extension

1. Click the extension icon in Chrome toolbar
2. The side panel should open
3. You should be prompted to authenticate with Google (first time only)
4. After authentication, you can start configuring the extension

## Platform-Specific Installation (Chrome vs Chromium/Raspberry Pi)

The extension works differently on **Chrome (Windows/Mac)** vs **Chromium (Linux/Raspberry Pi)** due to differences in OAuth support.

### Chrome (Windows/Mac) - Standard Installation

- Uses OAuth via `chrome.identity.launchWebAuthFlow()` and caches tokens in extension storage
- Works well with a **Web application** OAuth client ID + `chromiumapp.org` redirect URIs

### Chromium (Linux/Raspberry Pi) - Requires OAuth Modification

Chromium on Raspberry Pi frequently fails with `chrome.identity.getAuthToken()` (‚ÄúThe user is not signed in.‚Äù), even after logging in. This project uses a **Chromium-safe** OAuth flow via `chrome.identity.launchWebAuthFlow()` and cached tokens.

**Steps:**

1. In Google Cloud Console ‚Üí **APIs & Services** ‚Üí **Credentials**, create a **NEW OAuth 2.0 Client ID**:
   - Application type: **Web application** (not Chrome extension)
   - Name: "Savvy Pirate Web"
   - Authorized redirect URIs:
     - `https://<EXTENSION_ID>.chromiumapp.org/`
     - If you ever get a *new* extension ID, add another redirect URI (you can have multiple)
   - Get the extension ID from `chrome://extensions` after loading the extension

2. Update `manifest.json` with the new **Web application** client ID

3. Reload the extension and authenticate via the OAuth popup that appears

#### IMPORTANT: Extension ID stability (avoid changing OAuth redirect URIs)

Your OAuth redirect URI uses the extension ID:

- `https://<EXTENSION_ID>.chromiumapp.org/`

To keep the **same extension ID** on the Pi:

- Always load the extension from the **same folder path** (shown in `chrome://extensions` ‚Üí Details ‚Üí ‚ÄúLoaded from‚Äù)
- When updating code, **copy files into that same folder** and click **Reload** (do not ‚ÄúLoad unpacked‚Äù again)

## Raspberry Pi VNC Setup for Headless Operation

If you‚Äôre running the extension on a Raspberry Pi 24/7, VNC lets you remotely manage Chromium + the extension from your development PC.

### On the Raspberry Pi:

1. **Enable VNC via raspi-config:**

```bash
sudo raspi-config
# Navigate to: Interface Options ‚Üí VNC ‚Üí Enable
```

2. **Or install RealVNC Server manually:**

```bash
sudo apt update
sudo apt install realvnc-vnc-server realvnc-vnc-viewer
sudo systemctl enable vncserver-x11-serviced
sudo systemctl start vncserver-x11-serviced
```

3. **Find Pi's IP address:**

```bash
hostname -I
```

4. **Set a static IP (recommended):**
   - Edit `/etc/dhcpcd.conf` or configure via your router's DHCP reservation

### On your PC (Windows/Mac):

1. Download and install [RealVNC Viewer](https://www.realvnc.com/en/connect/download/viewer/)
2. Connect using: `<PI_IP_ADDRESS>:5900` (e.g., `192.168.3.232:5900`)
3. Login with your Pi username and password

### First-Time Extension Setup via VNC:

1. Open Chromium on the Pi
2. Go to `chrome://extensions` ‚Üí Enable Developer Mode ‚Üí Load unpacked
3. Complete Google OAuth sign-in (one-time)
4. Login to LinkedIn (one-time)
5. Configure schedules and sources
6. Extension will run 24/7 even when VNC is disconnected

## Deploying Code Updates to Raspberry Pi

When you make changes on your development machine, you can push updated extension code to the Pi and reload the extension in Chromium.

### Recommended: use the included deploy scripts (keeps the same extension ID)

This repo includes:

- `deploy-to-pi.sh` (bash; runs scp/ssh)
- `Deploy-to-Pi.bat` (one-click Windows wrapper; runs `deploy-to-pi.sh --files-only`)

#### Key rule (to avoid a new extension ID)

Deploy updates into the **same folder** Chromium is already using on the Pi:

- On the Pi, open `chrome://extensions` ‚Üí Savvy Pirate ‚Üí Details
- Confirm: **Loaded from: `~/extensions`**
- Ensure `PI_EXTENSION_PATH="/home/savvy-pirate/extensions"` in `deploy-to-pi.sh`

#### Typical workflow

1. On Windows, run `Deploy-to-Pi.bat` (copies files only)
2. On the Pi, open `chrome://extensions`
3. Click **Reload** on the existing Savvy Pirate extension

### deploy-to-pi.sh options

From Git Bash on Windows:

```bash
./deploy-to-pi.sh --files-only   # copy files only (recommended default)
./deploy-to-pi.sh --quick        # copy frequently edited files + restart chromium
./deploy-to-pi.sh --restart      # restart chromium only
./deploy-to-pi.sh --status       # show chromium status + extension folder listing
```

### Branch workflow (deploy from the branch you‚Äôre working on)

If you‚Äôre developing on a feature branch:

```bash
git checkout <your-branch>
git pull
./deploy-to-pi.sh --files-only
```

Then reload the extension on the Pi in `chrome://extensions`.

### Using SCP (Secure Copy)

**From Windows (PowerShell or CMD):**

```bash
scp -r "C:\Users\<USERNAME>\path\to\automated_scraper" <PI_USER>@<PI_IP>:~/extensions/
```

**Example:**

```bash
scp -r "C:\Users\russe\automated_scraper" savvy-pirate@192.168.3.232:~/extensions/
```

**From Mac/Linux:**

```bash
scp -r ~/path/to/automated_scraper <PI_USER>@<PI_IP>:~/extensions/
```

### After Deploying:

1. Connect via VNC
2. Go to `chrome://extensions`
3. Click the refresh icon on Savvy Pirate to reload the extension
4. Verify in service worker console that the new version loaded

### Optional: Create a Deploy Script

**Windows (deploy.bat):**

```batch
@echo off
scp -r "C:\Users\russe\automated_scraper" savvy-pirate@192.168.3.232:~/extensions/
echo Deployed! Remember to reload the extension in Chromium.
pause
```

**Mac/Linux (deploy.sh):**

```bash
#!/bin/bash
scp -r ~/automated_scraper savvy-pirate@192.168.3.232:~/extensions/
echo "Deployed! Remember to reload the extension in Chromium."
```

### SSH Access (for troubleshooting):

```bash
ssh savvy-pirate@192.168.3.232
```

## Usage

### First-Time Setup

1. **Load Input Sheet**
   - Open the extension side panel (click extension icon)
   - Enter your Google Sheet URL or ID in "Input Sheet" section
   - Click "Load"
   - Your input sheet should have columns: Source Connection (column A), Target Job Title (column B), LinkedIn Search URL (column C)
   - The input sheet URL is automatically saved and persists across sessions

2. **Add Output Workbook**
   - Click "+ Add Workbook" in "Workbook Mappings" section
   - Enter Google Sheet URL or ID
   - Optionally map to a specific source
   - Click "Add Workbook"

3. **Map Sources to Workbooks**
   - In "Workbook Mappings", select a workbook for each source
   - Data from each source will be saved to its mapped workbook
   - Each source can have its own workbook

### Manual Scraping

Manual scraping automatically navigates through all searches for a selected source:

1. Open the extension side panel
2. Go to "üîç Manual Scraping" section
3. Select a source from the dropdown
4. Click "‚ñ∂ Start Scrape"
5. The extension will:
   - Navigate to the first search URL for that source
   - Scrape all profiles from all pages
   - Automatically move to the next search URL
   - Continue until all searches are complete
6. Monitor progress in real-time:
   - Progress bar shows completed/total searches
   - Execution history shows live profile count updates
7. Click "‚èπ Stop" to stop scraping at any time
8. All scraping occurs in a single dedicated tab (no new tabs opened)

**Note**: You must be logged into LinkedIn. The extension uses a dedicated browser tab for all scraping operations.

### Scheduled Scraping

1. Open the extension side panel
2. Go to "‚è∞ Schedules" section
3. Click "+ Add Schedule"
4. Select:
   - Source name (Connection Source from Input Sheet)
   - Day of week
   - Time (hour and minute) - uses Eastern Time
5. Click "Save Schedule"
6. The schedule will run automatically at the specified time
7. Toggle schedules on/off using the switch
8. View execution history in "üìú Execution History" section
9. Execution history shows:
   - Date and time
   - Source name (clickable link to scraped data)
   - Status (running/completed/failed)
   - Profiles scraped (updates live during scraping)
   - Click on source name or profile count to open the Google Sheets tab

### Compare Tabs

Compare two date tabs to find new entries:

1. Open the extension side panel
2. Go to "üìä Compare Tabs" section
3. Select a workbook
4. Select baseline tab (older data) and compare tab (newer data)
5. Enter output tab name (e.g., "New_12_16")
6. Select compare key:
   - **Name**: Compare by person's name
   - **LinkedIn URL**: Compare by LinkedIn profile URL (recommended, more accurate)
7. Click "Compare Tabs"
8. New tab will be created with only the differential entries (entries in compare tab but not in baseline tab)

### Settings

**Zapier Webhook**:
1. Go to "‚öôÔ∏è Settings" section
2. Enter your Zapier webhook URL
3. Click "Save Webhook URL"
4. Click "Test Webhook" to verify (sends a test `schedule_started` notification)

**Webhook Notification Categories**:

All notifications include a `category` field for easy filtering in Zapier:

- **"status"**: Regular notifications (schedule_started, schedule_completed)
- **"error"**: Error notifications (schedule_failed, scrape_failed, error)

**Recommended Zapier Setup**:

1. Use **Paths** in Zapier to branch notifications
2. Path 1: Filter `category` equals `"status"` ‚Üí Route to Status Channel
3. Path 2: Filter `category` equals `"error"` ‚Üí Route to Error Channel

**All webhook payloads include**:
- `type`: Notification type
- `category`: "status" or "error"
- `sourceName`: Connection Source name from Input Sheet (column A)
- `timestamp`: ISO timestamp
- `data`: Type-specific data

## Data Formats

### Input Sheet Format

The input sheet should have three columns:

| Column A | Column B | Column C |
|----------|----------|----------|
| Source Connection | Target Job Title | LinkedIn Search URL |

Example:
```
Taylor Matthews | Financial Advisor | https://www.linkedin.com/search/results/people/?origin=FACETED_SEARCH&connectionOf=...
Jeff Nash | Wealth Manager | https://www.linkedin.com/search/results/people/...
```

- **Column A (Source Connection)**: The name of the person whose connections you're scraping
- **Column B (Target Job Title)**: The job title to search for
- **Column C (LinkedIn Search URL)**: The full LinkedIn People search URL

**Multiple rows with the same Source Connection name** will all be scraped together when that source is selected.

### Output Sheet Format

Each workbook automatically creates weekly tabs named `MM_DD_YY` (Eastern Time):

| Date | Name | Title | Location | Connection Source | LinkedIn URL | Accreditation 1-6 |
|------|------|-------|----------|-------------------|--------------|-------------------|
| YYYY-MM-DD | John Doe, CFP¬Æ | Financial Advisor | New York, NY | Taylor Matthews | https://... | CFA |

**Columns**:
1. **Date**: Date scraped (YYYY-MM-DD)
2. **Name**: Person's name (accreditations extracted to separate columns)
3. **Title**: Job title
4. **Location**: Location
5. **Connection Source**: Source Connection from Input Sheet
6. **LinkedIn URL**: Full LinkedIn profile URL
7. **Accreditation 1-6**: Professional accreditations (CFA, CFP¬Æ, MBA, etc.)

**Tab Naming**: Output tabs are automatically named `MM_DD_YY` (Eastern Time), e.g., `12_17_24` for December 17, 2024.

## Troubleshooting

### Extension Won't Load

- Check that `manifest.json` is valid JSON
- Verify all required files exist (service worker, content script, popup)
- Check Chrome console for errors
- Ensure you created `manifest.json` from `manifest-template.json`

### OAuth Errors

- Verify Extension ID matches in Google Cloud Console
- Check redirect URI format: `https://<extension-id>.chromiumapp.org/`
- Re-authenticate by clicking extension icon
- Ensure OAuth Client ID in `manifest.json` matches Google Cloud Console
- Verify OAuth consent screen is configured correctly

#### OAuth keeps prompting / ‚ÄúThe user is not signed in‚Äù (Raspberry Pi Chromium)

If you keep getting re-prompted to log in (or you see an auth error like **‚ÄúThe user is not signed in.‚Äù**) on the Pi:

1. On the Pi, open `chrome://extensions` ‚Üí Savvy Pirate ‚Üí copy the **ID**
2. In Google Cloud Console ‚Üí **APIs & Services ‚Üí Credentials**, confirm you are using a **Web application** OAuth client
3. In that same OAuth client, confirm **Authorized redirect URIs** includes:
   - `https://<EXTENSION_ID>.chromiumapp.org/` (replace with the ID from step 1)
4. Confirm the Pi‚Äôs deployed `manifest.json` uses that **Web application client ID** (not a Chrome extension client)
5. Reload the extension (`chrome://extensions` ‚Üí Reload) and try again

### Service Worker Inactive

- Click extension icon to wake it
- Check service worker console: `chrome://extensions` ‚Üí "service worker" link
- Service worker should automatically wake when extension is used

### Scraping Finds 0 Profiles

LinkedIn frequently changes their DOM structure. To fix:

1. Navigate to a LinkedIn search page
2. Open browser console (F12)
3. Copy the content of `linkedin-diagnostic.js` and paste in console
4. Review output to identify new selectors
5. Update `SELECTORS` object in `content/content.js`
6. Reload extension

### Duplicate Rows or Invalid Data

If you see duplicate rows or rows with empty names:

1. The extension validates all profiles before adding to queue
2. Invalid profiles (empty names, invalid URLs) are skipped
3. Duplicates are prevented by URL-based deduplication
4. Use the "Deduplicate" feature in the popup footer if needed

### Data Not Appearing in Sheets

1. Check queue status in popup footer
2. Check for failed items (will show in footer)
3. Click "Retry Failed" to retry failed items
4. Check service worker console for errors
5. Verify workbook permissions (extension needs edit access)
6. Check Google Sheets API quota in Google Cloud Console

### Schedule Not Triggering

- Verify schedule is enabled (toggle switch in Schedules section)
- Check timezone (schedules use Eastern Time)
- Check service worker console for schedule logs
- Verify alarms are created: See debugging commands below
- Schedule checker runs every minute, so may take up to 1 minute after scheduled time

### Webhook Not Working

- Verify webhook URL is correct
- Check that Zapier webhook is active
- Test using "Test Webhook" button in Settings
- Check service worker console for webhook errors
- Verify `https://hooks.zapier.com/*` is in host_permissions (already included)

### Execution History Not Updating

- Execution history updates automatically when profiles are scraped
- If profile count isn't updating, check service worker console
- Reload the extension side panel to refresh the view
- Execution history is limited to last 100 records

## Debugging

### Service Worker Console

1. Go to `chrome://extensions`
2. Find "Savvy Pirate"
3. Click "service worker" link (opens DevTools)

### View Storage

In service worker console:
```javascript
chrome.storage.local.get(null, data => console.table(data));
```

### Check Alarms

In service worker console:
```javascript
chrome.alarms.getAll(alarms => {
    console.log('Active alarms:', alarms.map(a => ({
        name: a.name,
        next: new Date(a.scheduledTime).toLocaleString()
    })));
});
```

### Test Message Handlers

In service worker console:
```javascript
// Test ping
chrome.runtime.sendMessage({action: 'PING'}, console.log);

// Get queue status
chrome.runtime.sendMessage({action: 'GET_QUEUE_STATUS'}, console.log);

// Get schedules
chrome.runtime.sendMessage({action: 'GET_SCHEDULES'}, console.log);

// Get execution history
chrome.runtime.sendMessage({action: 'GET_EXECUTION_HISTORY', limit: 10}, console.log);
```

### Content Script Test

On a LinkedIn page, open browser console (F12):
```javascript
chrome.runtime.sendMessage({action: 'PING'}, r => console.log('Content script alive:', r));
```

## File Structure

```
automated_scraper/
‚îú‚îÄ‚îÄ manifest.json              # ‚ö†Ô∏è DO NOT COMMIT - contains your OAuth Client ID
‚îú‚îÄ‚îÄ manifest-template.json     # ‚úÖ Template for creating manifest.json
‚îú‚îÄ‚îÄ .gitignore                 # Excludes manifest.json and sensitive files
‚îú‚îÄ‚îÄ background/
‚îÇ   ‚îú‚îÄ‚îÄ service_worker.js      # Main orchestration hub
‚îÇ   ‚îú‚îÄ‚îÄ auth.js                # OAuth token management
‚îÇ   ‚îú‚îÄ‚îÄ sheets_api.js          # Google Sheets API wrapper
‚îÇ   ‚îú‚îÄ‚îÄ sync_queue.js          # Local queue with retry logic
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.js           # Schedule management and execution history
‚îÇ   ‚îî‚îÄ‚îÄ notifications.js       # Zapier webhook notifications
‚îú‚îÄ‚îÄ content/
‚îÇ   ‚îî‚îÄ‚îÄ content.js             # LinkedIn DOM scraping
‚îú‚îÄ‚îÄ popup/
‚îÇ   ‚îú‚îÄ‚îÄ popup.html             # UI structure
‚îÇ   ‚îú‚îÄ‚îÄ popup.css              # Dark theme styling
‚îÇ   ‚îî‚îÄ‚îÄ popup.js               # UI controller
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ constants.js           # Shared constants
‚îú‚îÄ‚îÄ icons/
‚îÇ   ‚îú‚îÄ‚îÄ icon16.png
‚îÇ   ‚îú‚îÄ‚îÄ icon48.png
‚îÇ   ‚îî‚îÄ‚îÄ icon128.png
‚îú‚îÄ‚îÄ linkedin-diagnostic.js     # Selector maintenance tool
‚îî‚îÄ‚îÄ README.md                  # This file
```

## Log Prefixes

When debugging, look for these log prefixes in the service worker console:

- `[SW]` - Service worker (main orchestration)
- `[AUTH]` - Authentication
- `[QUEUE]` - Sync queue operations
- `[SHEETS]` - Sheets API calls
- `[SCHEDULE]` - Scheduling operations
- `[NOTIFY]` - Webhook notifications
- `[CS]` - Content script (in browser console on LinkedIn pages)
- `[POPUP]` - Popup UI

## Rate Limiting & Detection Prevention

### Current Throttling

The extension includes built-in anti-detection delays:

**Between Pages** (within a search):
- Random delay: **5-8 seconds** between pages
- Scroll wait: **2 seconds** for lazy loading

**Between Searches**:
- **Manual scraping**: 5-10 seconds
- **Auto-run/scheduled**: 30-60 seconds

**Other Timing**:
- Page load wait: 3-5 seconds
- Scroll wait: 2 seconds

### Recommendations for Safe Scraping

‚ö†Ô∏è **Important**: Even with a Pro/Recruiter account, LinkedIn can still detect automated scraping patterns. Account type doesn't exempt you from detection, but may provide higher rate limits.

**Best Practices**:

1. **Limit Searches Per Session**:
   - **Conservative**: 10-20 searches per hour
   - **Moderate**: 20-50 searches per hour  
   - **Aggressive** (not recommended): 50+ searches per hour

2. **Spread Out Scraping**:
   - Use scheduled scraping instead of running everything at once
   - Schedule different sources at different times
   - Avoid scraping the same source multiple times per day

3. **Respect Daily Limits**:
   - Limit total searches per day (recommended: < 100/day)
   - Take breaks between large scraping sessions

4. **Watch for Warning Signs**:
   - CAPTCHAs appearing frequently
   - "Something went wrong" errors
   - Account restrictions or warnings from LinkedIn
   - IP-based rate limiting

5. **Use Scheduled Scraping**:
   - Spread searches throughout the week
   - One source per day is safer than multiple sources in one day
   - Schedule during normal business hours (LinkedIn may be more lenient)

### Adjusting Throttling

You can increase delays in `utils/constants.js`:

```javascript
export const CONFIG = {
    MIN_WAIT_SECONDS: 5,      // Increase to 8-10 for more conservative
    MAX_WAIT_SECONDS: 8,      // Increase to 15-20 for more conservative
    // ...
};
```

For search delays, modify in `background/service_worker.js`:
- **Manual scrape**: Currently 5-10 seconds (line ~528)
- **Auto-run**: Currently 30-60 seconds (line ~415)

### Pro/Recruiter Account Considerations

**Advantages**:
- Higher search result limits
- More InMail credits
- Access to Recruiter tools

**Limitations**:
- **Does NOT prevent detection** of automated scraping
- LinkedIn still monitors for bot-like behavior patterns
- Account can still be restricted or banned for scraping

**Recommendation**: Treat your account with care regardless of type. The extension's throttling is designed to mimic human behavior, but excessive scraping can still trigger LinkedIn's anti-scraping systems.

## Security Notes

‚ö†Ô∏è **IMPORTANT**:
- `manifest.json` contains your OAuth Client ID - keep it secure
- **Do NOT commit `manifest.json` to public repositories**
- `.gitignore` is configured to exclude sensitive files
- Use `manifest-template.json` as a template for others
- OAuth Client ID allows access to Google Sheets - treat it as a secret

## Key Features Explained

### Dedicated Scrape Tab

The extension uses a single dedicated browser tab for all scraping operations. This tab is:
- Created once and reused for all scraping sessions
- Saved in extension storage
- Automatically navigated to new search URLs
- Prevents opening multiple tabs during scraping

### Live Execution History

Execution history updates in real-time during scraping:
- Profile count updates as profiles are scraped
- Source name is clickable to open the Google Sheets tab
- Profile count is also clickable when > 0
- Links work even while scraping is running

### Weekly Tab Management

Output workbooks automatically create weekly tabs:
- Tab names: `MM_DD_YY` (Eastern Time)
- Tabs are created on first scrape of the week
- Headers are automatically added
- Old tabs are preserved (not deleted)

### Multi-Layer Selector Strategy

The content script uses a fallback selector strategy:
- Primary selectors (most reliable)
- Fallback selectors (backup if primary fails)
- Validation ensures data quality
- Prevents scraping invalid profiles

### Queue Retry System

Failed syncs are automatically retried:
- Exponential backoff (2s, 4s, 8s, 16s, 32s)
- Maximum 5 retries
- Failed items moved to failed queue after max retries
- Manual retry available from popup footer

## Support

For issues or questions:
1. Check troubleshooting section above
2. Review service worker console logs
3. Use `linkedin-diagnostic.js` if scraping fails
4. Check Google Sheets API quota if data sync fails
5. Review execution history for detailed error information

## License

[Add your license here]

---

**Version**: 2.0.0  
**Last Updated**: December 2024
